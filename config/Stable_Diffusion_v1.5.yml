base_model_Stable_diffusion:
  name: Stable Diffusion V1.5
  description: Main V1.5 Stable Diffusion checkpoint
  files:
    - inpainting:
        url: "https://huggingface.co/camenduru/i15/resolve/main/sd-v1-5-inpainting.ckpt"
        path: "$SD15_MODEL_DIRECTORY$/sd-v1-5-inpainting.ckpt"
  default: True

RPG_V5:
  name: RPG V5
  Description: >
    This model card focuses on Role Playing Game portrait similar to 
    Baldur's Gate, Dungeon and Dragon, Icewindale, and more modern style of RPG
     character.


    documentation:https://huggingface.co/Anashel/rpg/resolve/main/RPG-V4-Model-Download/RPG-Guide-v4.pdf
    source:https://civitai.com/models/1116?modelVersionId=124626

  files:
      - RPG_V5_main:
          url: "https://civitai.com/api/download/models/124626?type=Model&format=SafeTensor&size=pruned&fp=fp16"
          path: "$SD15_MODEL_DIRECTORY$/RPG-V5.safetensors"
      - RPG_V4_inpaint:
          url: "https://civitai.com/api/download/models/96255?type=Model&format=SafeTensor&size=full&fp=fp16"
          path: "$SD15_MODEL_DIRECTORY$/RPG-V4-inpaint.safetensors"

Control_net:
  name: ControlNet
  description: >
    - control_sd15_canny.pth: The ControlNet+SD1.5 model to control SD
        using canny edge detection.

    - control_sd15_depth.pth model to control SD using Midas depth estimation.

    - control_sd15_hed.pth model to control SD using HED edge detection (soft edge).

    - control_sd15_mlsd.pth model to control SD using M-LSD line detection 
    (will also work with traditional Hough transform).

    - control_sd15_normal.pth model to control SD using normal map. 
    Best to use the normal map generated by that Gradio app. Other normal 
    maps may also work as long as the direction is correct (left looks red,
    right looks blue, up looks green, down looks purple).

    - control_sd15_openpose.pth model to control SD using OpenPose pose 
    detection. Directly manipulating pose skeleton should also work.

    - control_sd15_scribble.pth The ControlNet+SD1.5 model to control SD using 
    human scribbles. The model is trained with boundary edges with very strong
    data augmentation to simulate boundary lines similar to that drawn by human.

    - control_sd15_seg.pth model to control SD using semantic segmentation. 
    The protocol is ADE20k.
    
    https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main

    Source:https://huggingface.co/lllyasviel/ControlNet
  files:
    - control_depth:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_depth-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/control_sd15_depth.safetensors"
    - control_canny:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_canny-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/control_sd15_canny.safetensors"
    - control_mlsd:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_mlsd-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/control_sd15_mlsd.safetensors"
    - control_hed:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_hed-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/control_sd15_hed.safetensors"
    - control_normal:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_normal-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/control_sd15_normal.safetensors"
    - control_openpose:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_openpose-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/control_sd15_openpose.safetensors"
    - control_scribble:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_scribble-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/control_sd15_scribble.safetensors"
    - control_seg:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_seg-fp16.safetensors"
        path: $SD15_CONTROL_NET_DIRECTORY$/control_sd15_seg.safetensors
    - control_inpaint:
        url: "https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/diffusion_pytorch_model.fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/control_v11p_sd15_inpaint.safetensors"
    - control_tile:
        url: "https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1e_sd15_tile.pth"
        path: "$SD15_CONTROL_NET_DIRECTORY$/control_v11f1e_sd15_tile.pth"

  default: True

T2I_Adapter:
  name: T2I-Adapter
  description: >
    Official implementation of T2I-Adapter: Learning Adapters to Dig out
    More Controllable Ability for Text-to-Image Diffusion Models.


    T2I-Adapter, a simple and small (~70M parameters, ~300M storage space)
    network that can provide extra guidance to pre-trained text-to-image 
    models while freezing the original large text-to-image models.


    Source: https://github.com/TencentARC/T2I-Adapter

  files:
    - t2iadapter_canny:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_canny-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/t2iadapter_canny_sd15v2.safetensors"
    - t2iadapter_color:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_color-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/t2iadapter_color_sd14v1.safetensors"
    - t2iadapter_depth:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_depth-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/t2iadapter_depth_sd15v2.safetensors"
    - t2iadapter_keypose:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_keypose-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/t2iadapter_keypose-fp16.safetensors"
    - t2iadapter_openpose:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_openpose-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/t2iadapter_openpose_sd14v1.safetensors   "
    - t2iadapter_seg:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_seg-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/t2iadapter_seg_sd14v1.safetensors"
    - t2iadapter_style:
        url: "https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_style-fp16.safetensors"
        path: "$SD15_CONTROL_NET_DIRECTORY$/t2iadapter_style_sd14v1.safetensors"
    - t2iadapter_sketch:
        url: "https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd15v2.pth"
        path: "$SD15_CONTROL_NET_DIRECTORY$/t2iadapter_sketch_sd15v2.safetensors"
  default: True
  
Deliberate_v2:
  name: Deliberate v2
  description: >
    This model provides you the ability to create anything you want.

    The more power of prompt knowledges you have, the better results you'll get.

    It basically means that you'll never get a perfect result with just a few words.

    You have to fill out your prompt line extremely detailed. 


    Source: https://huggingface.co/XpucT/Deliberate

    source: https://civitai.com/models/4823/deliberate
  files:
    - main:
        url: "https://huggingface.co/XpucT/Deliberate/resolve/main/Deliberate_v2.safetensors"
        path: "$SD15_MODEL_DIRECTORY$/Deliberate_v2.safetensors"
    - inpainting:
        url: "https://huggingface.co/XpucT/Deliberate/resolve/main/Deliberate-inpainting.safetensors"
        path: "$SD15_MODEL_DIRECTORY$/Deliberate-inpainting.safetensors"
  default: True

Comic_Diffusion_V2.0:
  name: Comic Diffusion V2.0
  description: >
    Trained on 6 styles at once, it allows anyone to create unique but
    consistent styles by mixing any number of the tokens. Even changing
    the order of the same list influences results so there's a lot to
    experiment with here. This was created so anyone could create their
    comic projects with ease and flexibility.


    The tokens for V2 are:

    - **charliebo artstyle**

    - **holliemengert artstyle**

    - **marioalberti artstyle**

    - **pepelarraz artstyle**

    - **andreasrocha artstyle**

    - **jamesdaly artstyle**


    Source: https://huggingface.co/ogkalu/Comic-Diffusion
  files:
    - main:
        url: "https://huggingface.co/ckpt/Comic-Diffusion/resolve/main/comic-diffusion-V2.ckpt"
        path: "$SD15_MODEL_DIRECTORY$/comic-diffusion-V2.ckpt"
  default: False

Cyberpunk_Anime_Diffusion:
  name: Cyberpunk Anime Diffusion
  description: >
    This model generates cyberpunk anime characters. use keyword **dgs** in your 
    prompt, with illustration style to get even better results. For sampler,
    use Euler A for the best result (DDIM kinda works too), CFG Scale 7, 
    steps 20 should be fine. 


    Source: https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion

  files:
    - main:
        url: "https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/Cyberpunk-Anime-Diffusion.safetensors"
        path: "$SD15_MODEL_DIRECTORY$/Cyberpunk-Anime-Diffusion.safetensors"
  default: False

Inkpunk_Diffusion_v2:
  name: Inkpunk Diffusion v2
  description: >
    Finetuned Stable Diffusion model trained on dreambooth. Vaguely inspired
    by Gorillaz, FLCL, and Yoji Shinkawa. Use **nvinkpunk** in your prompts.


    Source: https://huggingface.co/Envvi/Inkpunk-Diffusion

  files:
    - main:
        url: "https://huggingface.co/Envvi/Inkpunk-Diffusion/resolve/main/Inkpunk-Diffusion-v2.ckpt"
        path: "$SD15_MODEL_DIRECTORY$/Inkpunk-Diffusion-v2.ckpt"
  default: False

SPYBG_Digital_Artists:
  name: SPYBG's Toolkit for Digital Artists
  description: >
    Tips for using my model:
    I would recommend using some of this settings, they provide the best
    results at least for me. But feel free to experiment.


    Sampler: DPM++2M Karras


    Steps: 150 steps (lower steps also work but for this training data 150
    works the best based on my testing)


    Recommended Resolution: 768x768 (The model i used as base for training 
    is a custom modified base of Protogen 3.4 merged with older versions of
    my toolkit (v2.0), and based on that I've trained my model with 768x768
    datasets so, I recommend to use 768x768 and 768x1280, or higher 
    resolutions).


    Note: with version 4.0 and above I've used the basic 1-5-pruned model and
    i've finetuned it properly


    CFG Scale: 5 ~ 7 works best


    Trigger words: **tk-char** (for characters) **tk-env** (for environments) 
    why tk? (tk stands for Toolkit)


    Source: https://civitai.com/models/4118/spybgs-toolkit-for-digital-artists

  files:
    - main:
        url: "https://civitai.com/api/download/models/17292?type=Model&format=SafeTensor&size=full&fp=fp16"
        path: "$SD15_MODEL_DIRECTORY$/SPYBG_Digital_Artists.ckpt"
  default: False

Dreamlike_Photoreal_v2:
  name: Dreamlike diffusion v2
  description: >
    Dreamlike Photoreal 2.0 is a photorealistic model based on Stable 
    Diffusion 1.5, made by dreamlike.art

    Warning: This model is horny! Add "nude, naked" to the negative prompt
    if want to avoid NSFW.

    You can add photo to your prompt to make your gens look more 
    photorealistic. Non-square aspect ratios work better for some prompts.
    If you want a portrait photo, try using a vertical aspect ratio.
    If you want a landscape photo, try using a horizontal aspect ratio.


    Source: https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0

  files:
    - main:
        url: "https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/dreamlike-photoreal-2.0.safetensors"
        path: "$SD15_MODEL_DIRECTORY$/dreamlike-photoreal-2.0.safetensors"
  default: False

Vivid_Watercolors:
  name: Vivid Watercolors
  description: >
    The model is trained with beautiful, artist-agnostic watercolor images
    using the midjourney method. The token is "wtrcolor style" It can be 
    challenging to use, but with the right prompts, but it can create
    stunning artwork.


    See an example prompt that I use in tests:


    > wtrcolor style, Digital art of (subject), official art, frontal, 
    smiling, masterpiece, Beautiful, ((watercolor)), face paint, 
    paint splatter, intricate details. Highly detailed, detailed eyes,
    [dripping:0.5], Trending on artstation by [artist]


    Source: https://civitai.com/models/4998/vivid-watercolors

  files:
    - main:
        url: "https://civitai.com/api/download/models/5762?type=Model&format=SafeTensor&size=full&fp=fp16"
        path: "$SD15_MODEL_DIRECTORY$/Vivid_Watercolors.safetensors"
  default: False


DreamShaper:
  name: DreamShaper
  description: >

    CLIP skip 2 on some pics, the model works with that too. 


    DreamShaper started as a model to have an alternative to MidJourney in 
    the open source world. I didn't like how MJ was handled back when I 
    started and how closed it was and still is, as well as the lack of freedom
    it gives to users compared to SD. Look at all the tools we have now from 
    Is to LoRA, from ControlNet to Latent Couple. We can do anything. 
    The purpose of DreamShaper has always been to make "a better Stable 
    Diffusion", a model capable of doing everything on its own, to weave dreams.

    Source: https:https://civitai.com/models/4384

  files:
    - main:
        url: "https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16"
        path: "$SD15_MODEL_DIRECTORY$/DreamShaper.safetensors"
    - inpaint:
        url: "https://civitai.com/api/download/models/131004?type=Model&format=SafeTensor&size=pruned&fp=fp16"
        path: "$SD15_MODEL_DIRECTORY$/DreamShaper.safetensors-inpaint"
  default: False


CharTurner:
  name: CharTurner
  description: >
    Character turnarounds helper for 1.5. trigger word: **charturnerv2**


    > "A character turnarounds of a (X) wearing (Y)", "Multiple views of
    the same characters" 


    controlNet works great with this. Charturner keeps the outfit 
    consistent, controlNet openPose keeps the turns under control.


    Source: https://civitai.com/models/3036?modelVersionId=8387
  files:
    - main:
        url: "https://civitai.com/api/download/models/8387?type=Model&format=PickleTensor&size=full&fp=fp16"
        path: '$SD15_TEXTUAL_INVERSION_DIRECTORY$/SD1.5_CharTurner.pt"'
  default: False
